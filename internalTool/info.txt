it is an observability tool that helps us in identifying system related issues, perform rca,
triage complex issue and finding out rca

The system processes ingested log bundles associated with specific Jira tickets, identifies anomalies in
system metrics and log entries within defined timeframes, and leverages SupportGPT to generate contextual summaries, potential Root Cause
Analysis (RCA), and retrieve relevant historical issues. The primary objective is to significantly reduce
troubleshooting time for support engineers by providing Al-driven insights derived from extensive log and metric data.

FUNCTIONALITIES:

1 Log ingestion:
1.1 user to provide log bundle path
1.2 automatically process log bundle as soon as it is available

2 Log browsing:
all logs in one place, allows user tp service, node, ip  and time duration to see aggregate data from

3 Log analysis:
3.1 1 click rule addition
3.2 flag known issues
3.3 real time browsing of critical logs, heatmap
3.4 health of services

4 AI/ML:
RCA summary
ai powered metrics
support gpt integration

rules in our tool is basically a combination of properties from a log line like 
    log_level, log_message,source_file, remediation, cause, problem

APIS:
dont use metadata api call, it can cause confusion while explaining the project
GET /logs/metadata
params: log_type,log_path
response: log_bundle_path,is_analyzed,log_bundle_size,log_bundle_id,log_type

POST /logs/analyze
params: log_bundle_path
response: status: inprogress, log_bundle_id, error_msg

GET /logs/job_status
params: log_bundle_id
response: id, job_status,start_time,end_time

what is actual use of haproxy?
receives all requests and based on request header, if present sends to worker vm else routes to itself

consumers consume message from rabbitmq by creating a connection to the queue and set the prefetch count to 1 to process only one log 
bundle at a time

consumers run as daemon process in worker vm
A daemon is a background process that runs continuously and performs a specific task without user interaction. 
How systemd Manages Daemons:
systemd starts, stops, and monitors daemons using service unit files 

Issues:
1. only one log bundle at a time,waiting state in rabbitmq
it is because current architecture use all cpus and parse logs in 10 minutes

2.load balancing between rabbitmq is not good, it is round robin, if node 3 is supposed to handle a bundle but it is processing
while node1 is free, we will wait for node3 to complete

3. db backup

4. round robin assigns to down vm

5. queries are to be optimised, millions of rows
data is stored in disk, manticore brings them to memory in form of batch with max size specified in max_batch and process it

6. all logs(debug/info/fatal) are processed,  info are almost half of the logs, can they be ignored


table in manticore contains:

id, message, filename_without_ext,filename,filepath,time,log_level,source_log_filename,pid,cvm_ip,anomaly,line_number
indexed on message,log_bundle_id

postgres metadata table:
log_bundle_id, log_bundle_path,job_status,start_time,end_time, hosting_worker_vm_ip,bundle_size,
bundle_type,is_analyzed,remote_log_bundle_path

manticore runs on a respective worker vm and mounts volume from a common fileserver, but this increases latency 
due to network calls to file server

Use Elasticsearch if:
‚úÖ You need scalable, distributed full-text search (handles TBs of data).
‚úÖ You require real-time analytics (e.g., log aggregation, APM, monitoring).
‚úÖ You need complex querying (e.g., aggregations, fuzzy search, geo-search).
‚úÖ You want a large ecosystem (e.g., Kibana, Logstash, Beats).
‚úÖ You can afford the higher resource usage (RAM, CPU, storage overhead).
üîπ Best for:
* Search engines (e-commerce, site search, document retrieval).
* Log & event analytics (e.g., ELK stack for observability).
* Geospatial search (e.g., delivery, ride-hailing apps).

Use ManticoreDB if:
‚úÖ You need lightweight and high-speed full-text search (faster than Elasticsearch for many queries).
‚úÖ You want lower resource consumption (better for limited hardware).
‚úÖ You require SQL-based querying (supports MySQL-compatible queries).
‚úÖ You want real-time indexing without significant overhead.
‚úÖ You need fast phrase searches with ranking optimizations.
üîπ Best for:
* Low-latency full-text search (e.g., chat apps, forums).
* SQL-based applications needing search capabilities.
* Embedded search inside MySQL or PostgreSQL applications.


* ManticoreDB has native SQL support (MySQL-compatible). It allows you to run full-text searches directly within a SQL database.
* Elasticsearch SQL is an abstraction on top of its JSON-based query DSL. It‚Äôs useful but not a full replacement for relational databases.

Limitations of Elasticsearch SQL
‚ùå No real relational joins ‚Äì Only "join-like" operations via nested fields or parent-child relationships.
‚ùå No transactions or ACID compliance ‚Äì Unlike traditional SQL databases.
‚ùå Limited update capabilities ‚Äì Elasticsearch is optimized for search, not frequent updates.
‚ùå Not all SQL functions are supported ‚Äì Some advanced SQL features like WINDOW functions are missing.




HaProxy vs nginx
When to Use Which?
* Choose¬†HAProxy¬†if:
    * You need the best performance for load balancing.
    * Your focus is on distributing traffic across backends.
    * Advanced traffic routing and high availability are critical.
* Choose¬†NGINX¬†if:
    * You need a versatile tool for web serving, reverse proxying, and caching.
    * SSL termination and serving static content are part of your requirements.
    * You want an easier configuration experience for simple setups.
Health Checks
* HAProxy:
    * Provides¬†robust, built-in health checks¬†at both Layer 4 and Layer 7.
    * More granular control over backend server health monitoring and failure recovery.
* NGINX:
    * Health checks are available in the open-source version but are less comprehensive compared to HAProxy.
    * Advanced health check features (e.g., active health checks with custom thresholds) are typically part of¬†NGINX Plus¬†(the commercial version).

HAProxy tends to have a lower memory footprint than Nginx, especially when acting purely as a TCP/HTTP load balancer without serving static content.

Use HAProxy if your primary goal is simple load balancing with header-based routing. HAProxy is more lightweight and optimized for this.
Use NGINX if you might later need caching, advanced request manipulation, or serving static content.


Key Reasons Why HAProxy is Better for SSL Termination
1Ô∏è‚É£ HAProxy Uses a Multi-Threaded Model (Better CPU Utilization)
* HAProxy uses multi-threading, allowing it to fully utilize multiple CPU cores for SSL decryption.
* Nginx (by default) uses a single-threaded event loop per worker process, which can cause CPU bottlenecks under heavy SSL traffic.
* Result: HAProxy scales SSL termination better across multiple cores, reducing CPU bottlenecks.
2Ô∏è‚É£ HAProxy Has Faster SSL Handshake Handling
* HAProxy optimizes session resumption, reducing the number of full SSL/TLS handshakes.
* It efficiently reuses TLS Session Tickets and Session IDs, reducing CPU usage.
* Nginx also supports session caching, but HAProxy handles it more efficiently under high traffic.
3Ô∏è‚É£ HAProxy Supports Asynchronous SSL Processing
* HAProxy can offload SSL processing to separate threads, reducing the load on the main request-processing thread.
* Nginx processes SSL in the main worker thread, making it more CPU-intensive.
* Result: HAProxy can handle more SSL connections with lower CPU overhead.
4Ô∏è‚É£ HAProxy Uses Built-in OpenSSL Optimizations (AES-NI)
* HAProxy is designed to work with OpenSSL optimizations like AES-NI, providing hardware-accelerated encryption.
* It has better integration with OpenSSL's multi-threaded engine, reducing CPU load.
* Nginx requires manual tuning to achieve similar optimizations.
5Ô∏è‚É£ HAProxy Has More Efficient Connection Management
* HAProxy keeps TCP connections alive longer, reducing the need for frequent SSL handshakes.
* It supports TLS Fast Open (TFO) and Keep-Alive tuning, which Nginx lacks by default.
* This reduces CPU usage, especially in high-traffic environments.



When to Use TimescaleDB ‚úÖ
‚úî Best for Long-Term Storage & Complex Queries:

PostgreSQL-based, optimized for time-series data.
SQL support makes it easy for data analysis.
Efficient data compression (up to 90% storage savings).
‚úî Handles Complex Time-Series Use Cases:

Ideal for IoT, financial data, event logging, and analytics.
Supports joins, aggregates, and more advanced queries that Prometheus struggles with.
‚úî Better for Retention & Historical Analysis:

Unlike Prometheus, it can store years of data efficiently.
Continuous Aggregates for downsampling data over time.
‚úî Better for High Cardinality Data:

Prometheus struggles with high cardinality (e.g., millions of unique labels).
TimescaleDB handles high-cardinality workloads more efficiently.


LuaScript

LuaScript refers to scripting using the Lua programming language, often used to extend applications, add automation, or enable dynamic behavior.
Lua is a lightweight, high-performance scripting language designed for embedding in larger applications. It is:
‚úî Fast ‚Üí Small memory footprint, optimized for speed.
‚úî Embeddable ‚Üí Used in games, databases, and networking tools.
HAProxy supports Lua scripting to modify requests dynamically, such as:
‚úî Custom request routing
‚úî Header manipulation
‚úî Advanced logging


kafka vs rabbitmq

Since your system has 3 worker VMs processing tasks that take time, you need:
‚úî Reliable task distribution
‚úî Message acknowledgment & retries
‚úî Efficient handling of long-running tasks

üîπ üëâ RabbitMQ is the better choice because:
1Ô∏è‚É£ Push-based message delivery ‚Üí Ensures tasks are assigned quickly to available workers.
2Ô∏è‚É£ Automatic retries ‚Üí If a worker fails, RabbitMQ can requeue the task.
3Ô∏è‚É£ Worker-level concurrency control ‚Üí Each worker can handle one task at a time using prefetch limits.
4Ô∏è‚É£ Better handling of long-running tasks ‚Üí A worker will only acknowledge a task when it's fully processed.

üìå Kafka is better for real-time event streaming, but RabbitMQ is better for your task queue because of worker-based task distribution and retries.


rabbitmq workflow:

so create a connection, then create a channel from that connection, declare an exchange and its type, declare a queue and  bind it to exchange using routing key, publisher
publish the message to that exchange with routing key, which gets published to that queue and consumed by that consumer


rabbitmq health checks check if worker vm rabbitmq connection is active or not and manticore is running on that vm or not

what if manticore completes the processing but fails to update the postgres db
    1. use backoff retry mechanism
    2. create a table in manticore that tracks if postgres update is successful
        CREATE TABLE update_queue (
        id INT,
        status STRING,
        retry_count INT
        );
    3. after retry exceeds a threshold, update grafana to trigger alerts

What if manticore goes down in between processing
    in this case, worker vm is not sending the ack back to rabbitmq, the tcp connection times out and message is requed
    Worker VM can send alerts to Grafana when Manticore is down and retries have failed multiple times


What Happens If the Whole Worker VM Crashes?
    RabbitMQ uses heartbeats, it detects when a worker is down
    If the entire Worker VM crashes (not just Manticore), the RabbitMQ connection is lost, and:
    The message remains unacked.
    RabbitMQ detects the broken connection and requeues the message.
    Another healthy worker picks it up.

What if rabbitmq is down?
    it is restarted by systemd with exponential backoff
    and now if metadata api call comes, response is sent from postgres
    if analyze api call comes, it reads from postgres db and then puts in queue, if it is down error is shown to ui

how idempotency of api is maintained?
    what if someone sends multiple analyze call in parallel
    1. from frontend side, analyze button is disabled
    2. each analyze api call inserts metadata in postgres with remote log bundle path as primary key which is unique
    and then message is published in rabbitmq

Redis:
8.0

manticore:
6.3.4

Rabbitmq:
3.13.4

which health checks are running?
1. panacea services health: nutanix_log_ingestion, nutanix_log_analyzer
2. root partition usage(disk usage)
3. low cpu Idle 
4. low memory check
5. manticore health and dir storage check

how does rabbitmq detects down consumer?

1. consumer level heartbeat, which is per hour, if consumer doesnt respond, connection is considered to be dead 
2. if consumer dies in between processing, rabbitmq detects connection failure and reque the message