Functional requirements:
1. Users can create alerts: symbol, condition type (>=, <=, % change, volume spike), threshold, frequency (one-time or recurring), expiration time.
2. Users can manage alerts: update, delete, list.
3. Real-time ingestion of market prices (per symbol).
4. Evaluate alerts when prices update and trigger notifications.
5. Deliver notifications over multiple channels: WebSocket (in-app), mobile push, email, SMS.

Non Functional requirements:
1. Latency: real-time path (price → evaluation → push to websocket) < 200 ms end-to-end for active users on hot symbols. (Configurable).
2. Throughput: handle high message rates from market feeds and tens of thousands of notifications/sec.
3. Availability: 99.95% (SLA) for core alerting service.
4. Consistency: eventual consistency acceptable for non-critical metadata; alert evaluation must be monotonic and idempotent.

capacity estimations:
2M dau, 20% have daily alerts set
average alerts per active user = approx 3 -> 1.2M alerts
alerts fired per sec 1.2*10^6/10^5 120 alerts per second
total number of alerts 1.2M daily
lets say 10000 total stocks, average update rate per stock-> 10^4 qps

Database:
prices(symbol TEXT, ts TIMESTAMPTZ, price NUMERIC, volume BIGINT, day_open NUMERIC, PRIMARY KEY (symbol, ts))

rule(
  rule_id BIGSERIAL PRIMARY KEY,
  user_id BIGINT NOT NULL,
  symbol TEXT NOT NULL,
  metric ENUM('price','open','ma_5','ma_20','ema_12','rsi_14','vwap'),
  comparator ENUM('lt','eq','gt','crosses_up','crosses_down','between'),
  threshold_low NUMERIC NULL,   -- for eq/gt/lt uses threshold_low; for between uses both
  threshold_high NUMERIC NULL,
  window_spec JSONB NULL,       -- optional: period=7d, granularity=1m, etc.
  hysteresis NUMERIC DEFAULT 0, -- e.g., 0.5% band
  cooldown_sec INT DEFAULT 600,
  active BOOLEAN DEFAULT TRUE,
  created_at TIMESTAMPTZ, updated_at TIMESTAMPTZ
);

rule_state(rule_id BIGINT PRIMARY KEY,
           last_eval_ts TIMESTAMPTZ,
           last_state BOOLEAN,              -- true=condition currently satisfied
           last_fire_ts TIMESTAMPTZ,
           suppressed_until TIMESTAMPTZ);   -- cooldown end

alerts(alert_id BIGSERIAL, rule_id BIGINT, user_id BIGINT, symbol TEXT,
       fired_ts TIMESTAMPTZ, value NUMERIC, metric TEXT, comparator TEXT,
       dedupe_key TEXT UNIQUE, delivered BOOLEAN, channel TEXT, details JSONB)

CREATE TABLE metric_events (
    symbol TEXT,
    metric TEXT,
    window TEXT,            -- e.g. "1m", "5m", "10m"
    event_time TIMESTAMP,   -- bucketed time, acts as clustering
    value DOUBLE,
    PRIMARY KEY ((symbol, metric, window), event_time)
) WITH CLUSTERING ORDER BY (event_time DESC); cassandra 

How does flink maintains state based on different rules?
Predefine common windowed metrics:
For example, compute for every symbol:
    Price bars: 1m, 5m, 15m, 1h, 1 day
    Moving averages: MA(5), MA(10), MA(20), MA(50), MA(200)
    RSI(14), RSI(7), price_bar
    VWAP per 1m, 5m, 1h



So Flink job maintains multiple sliding/tumbling windows in parallel, keyed by symbol.
When a user sets a rule (“drop >5% in last 10 minutes”), you can evaluate it using the 1m price bars → compute change over last 10 bars.

Works well because most trading windows are standardized (5m, 15m, 1h, daily).
Downside: if users demand arbitrary windows (like 17 minutes), you either approximate to nearest standard window or increase computation cost.


What does flink emit and who consumes it?
flink emits events like:
MetricEvent {
    symbol: "AAPL",
    metric: "RSI_14",
    value: 67.2,
    window: "10m",
    timestamp: 2025-08-31T15:00:00Z
}
this event is consumed by rule evaluator, it has per symbol a map: Map<Symbol, Map<Metric, MetricIndex>>
For each Symbol, you can track multiple metrics (RSI, MovingAverage, Volume, VWAP, etc.).
MetricIndex
    MetricIndex is usually a helper structure that stores the rolling window state for that metric.
    It can maintain:
        The raw values (or compressed form, like a ring buffer) needed to recompute the metric.

How do we persist rule evaluator state ?
store events in db and if evaluator crashes, it can rebuild the map from db
